[
  {
    "objectID": "media/Dolendar_System_Requirements.html",
    "href": "media/Dolendar_System_Requirements.html",
    "title": "Table of Content",
    "section": "",
    "text": "Dolendar System Requirements\nRuilin Wu\n\n\nTable of Content\n\n\n\n\n\n\n\n\nTable of Content\n\n\n\n\n\n\nIntroduction\nOutlines the purpose and overview of the Requirements Specification.\n2\n\n\nExecutive Summary\nA concise recap of the Dolendar‚Äôs key aspects and its overarching goals.\n3\n\n\nApplication Context / Environmental Constraints\nDetails of the application and the operating platforms.\n3-4\n\n\nFunctional Requirements\nSpecify the essential actions and features the application must provide.\n4-7\n\n\nNon-functional Requirements\nDetail the performance and quality for system designers and programmers.\n8\n\n\nOther Requirements\n\n9\n\n\nGlossary\nAdditional requirements still need to be mentioned.\n\n\n\nDefines specialized terms throughout the document.\n\n9\n\n\nAssumptions / Risks\nPotential uncertainties and challenges that could impact Dolendar.\n10\n\n\nPriorities / Implementation Phases\nOrder and timeline for the application‚Äôs development stages\n10\n\n\nFuture Directions and Expected Changes\nHints at the application‚Äôs evolution and potential updates\n11\n\n\n\n\n\n\n\n\nIntroduction\nThis document outlines the software requirements for Dolendar, a productivity suite of software developed by the INF43 Software Engineering group under the commission of Dom Dones, president and founder of DoMore Software Corp.\nDolendar integrates a calendar application and a to-do list manager, aimed at helping users enhance work productivity and personal time management, and understand the impact of procrastination on their scheduling.\n\n\nExecutive Summary\nDolendar is a sophisticated productivity tool that redefines traditional scheduling by integrating to-do lists with calendar functions. It allows users to craft detailed to-do lists while aligning and displaying their tasks alongside scheduled events, providing a holistic view of their commitments. This feature encourages frequent interaction with the calendar, fostering a proactive approach to task management and highlighting the effects of unfinished tasks on overall schedules. By clearly showing how delays can impact one‚Äôs time, Dolendar differentiates itself from ordinary organizational software, serving not just as a work aid but as a comprehensive system for managing personal goals and daily activities.\nDolendar offers both a personal-use version and an enterprise version. The personal version is free and equipped with all basic features, and a subscription version can support data storage and synchronization. Businesses can opt for the enterprise version, which offers additional functionalities tailored to professional needs and is accessible through corporate engagement with DoMore Software.\nThe design philosophy behind Dolendar prioritizes simplicity and user-friendliness, recognizing the necessity for an intuitive user experience amidst a competitive market. It is meticulously developed to meet high standards of usability, privacy, and data protection, thereby ensuring reliability for users. Designed for seamless integration with current digital ecosystems, Dolendar complements existing calendar applications and is compatible with cloud services, allowing immediate productivity enhancements without significant changes to existing workflows.\nDolendar goes beyond scheduling; it‚Äôs an ally in time management, bridging the gap between digital efficiency and the intricacies of everyday living, enabling users to reclaim control over their time with confidence and ease.\n\n\nApplication Context / Environmental Constraints\nDolendar software is crafted to serve many environments, demonstrating versatility and adaptability.\n\nIt is designed to streamline and enhance the productivity of business and office settings, where individuals and teams alike can efficiently manage their schedules, meetings, and collaborative endeavors. Within these settings, the software will facilitate team-based functionality, allowing for an intuitive interface where the team‚Äôs admin can assign tasks and make announcements with ease.\n\nDolendar extends its utility to personal use, where users can organize and track their individual goals, daily tasks, family events, and various other personal activities, making it an indispensable tool in the realm of personal organization.\n\nIt supports an on-the-go lifestyle, enabling users to access and manage their schedules and tasks with the same efficiency outdoors or while traveling.\n\nThe developmental blueprint for Dolendar spans across mobile and desktop platforms. The mobile application is being developed to serve both iOS and Android operating systems for phones and tablets. This version promises a responsive design, capable of adapting to a multitude of screen sizes and orientations for optimal functionality. Following the mobile release, a desktop application is set to be introduced to serve users on Windows, macOS, and Linux, catering to the needs of the desktop environment.\nWith regards to operating system support, the mobile application will be compatible with the system version for iOS and Android at the time of the application release and one prior. The desktop version will match up to the prevailing system versions of Windows, macOS, and Linux at the time of its release, ensuring broad accessibility.\nDesign considerations for Dolendar are centered around a modern, elegant, and sleek user interface (UI) design, emphasizing ease of use and intuitive interactions to reduce friction for the end-user. The UI will be inclusive, accommodating both dark and light modes with customizable color themes to not only cater to user preferences but also to ensure accessibility for users who are color-blind.\nCertain constraints dictate the software‚Äôs architecture to ensure it aligns with contemporary expectations. It is imperative that the software syncs with prevalent calendar services such as Google Calendar and Apple iCloud Calendar, negating the need for a proprietary calendar system within Dolendar. Emphasis is placed on adopting modern, secure, and robust programming languages and frameworks to fortify the software‚Äôs security, efficiency, and maintainability. A cloud-based backend service is also in the blueprint, designed to manage the intricacies of syncing and collaboration features in line with a subscription-based business model.\nIn the selection of programming languages, the INF43 Software Engineering group has been granted the liberty to choose the most fitting language for the development of this application, ensuring the final product is built on a foundation that maximizes its potential.\n\n\nFunctional Requirements\nDolendar is a combination of a to-do list app and a calendar, and it is designed to enhance work productivity and personal time management. It integrates various functionalities to address human psychology and procrastination, aiding users in achieving personal and professional goals.\nCore Functionalities\n\nEvent and Task Management: Users can create, modify, and delete tasks and events. Tasks are the user‚Äôs to-do items, activities or goals planned to be completed; events are planned events, such as meetings or courses, fixed schedules or flexible events. This feature includes setting titles, descriptions, and other relevant details for tasks and events. Tasks are more flexible and can be adjusted around fixed events in the calendar.\n\nTime Allocation for Tasks: Users assign an estimated duration to each task, which the system then integrates into the user‚Äôs calendar. This will allow users to have their tasks and events intersected, which helps visualize the time allocated to each task and manage the day more efficiently.\n\nDynamic Task Adjustment: Based on the user‚Äôs progress and upcoming deadlines, tasks dynamically change in the calendar. If a task is not completed within the expected time, it is automatically rescheduled, affecting the scheduling of future tasks. This feature highlights the impact of procrastination and helps users better manage their time.\n\nIf a user marks a task as done before the deadline, it will not affect subsequent tasks; however, if a task exceeds the deadline, all subsequent tasks will be delayed accordingly. Once an overdue task is completed, all subsequent tasks return to their normal schedule at the current time.\n\n\nSub-tasks Creation and Management: Allows breaking down large tasks into smaller, more manageable sub-tasks. Each sub-task can have its own description, duration, and completion status. This feature aids in organizing complex tasks and tracking progress on individual components.\n\nCalendar and To-do List Views: Separate views for the calendar and the to-do list, providing different perspectives on the user‚Äôs schedule. The calendar view displays both tasks and events chronologically, while the to-do list view focuses on tasks alone. Users can toggle between views based on their preference or need.\n\nTask Tags and Filters: Users can tag tasks and apply filters based on time windows and tags. This feature enhances the organization of tasks by categorizing them (like ‚Äúwork‚Äù, ‚Äúpersonal‚Äù, etc.) and allows users to focus on specific types of tasks within certain time frames using filters.\n\nSyncing: The free version of the software stores data exclusively on the local device and lacks the ability to synchronize across multiple devices. However, the personal paid version or the enterprise edition includes a syncing feature, allowing users to access their personalized settings and information on different devices.\n\nTeam Collaboration: Designed for team use, this feature enhances task distribution and synchronization among team members.\n\nTask Assignment and Notification:\n\nTeam admins can assign tasks to members.\n\nMembers receive notifications and see these tasks integrated into their personal calendars.\n\n\nSynchronization:\n\nThe sync feature ensures that updates made to tasks are immediately reflected on all team members‚Äô devices. This promotes effective collaboration and keeps the team aligned.\n\n\nPrivacy of Tasks and Events:\n\nTasks and calendar items can be set as either public or private. Private tasks or events are visible in the calendar as occupied time slots, but their content remains confidential. Team members can see that there is a private task at a certain time but cannot view its specifics.\n\n\nVisibility and Access:\n\nEvery team member has visibility into everyone else‚Äôs task list and calendar. Private tasks or events are visible in the calendar as occupied time slots, but their content remains confidential.\n\nTeam members can see that there is a private task at a certain time but cannot view its specifics.\n\nThis includes viewing the completion status of tasks.\n\nAny team member can add an item to another member‚Äôs task list once a day, further enhancing collaborative planning and task distribution.\n\n\n\nUse Case:\nUse Case 1: Managing Personal Tasks (Primary Actor: Individual User)\nBasic Flow:\n\nUser logs into Dolendar.\n\nUser creates a new task, assigning a title, description, and estimated duration.\n\nThe system integrates the task into the user‚Äôs calendar based on available time slots.\n\nUser marks the task as completed after finishing it.\n\nSystem updates the calendar, moving upcoming tasks forward.\n\nAlternative Flow:\n\nUser attempts to add a task with an unrealistic duration.\n\nSystem prompts a warning about the duration conflicting with other scheduled events.\n\nUser adjusts the task duration or reschedules other events.\n\nException Flow:\n\nUser forgets to mark a task as completed.\n\nSystem keeps the task in the calendar, causing a shift in subsequent tasks.\n\nUse Case 2: Collaborative Task Management (Primary Actor: Team Member; Secondary Actor: Team Admin)\nBasic Flow:\n\nTeam Admin logs into Dolendar and accesses the team calendar.\n\nTeam Admin assigns a task to a Team Member, specifying details and deadline.\n\nThe system notifies the Team Member and adds the task to their personal calendar.\n\nTeam Member completes the task and marks it as done.\n\nSystem updates the team calendar showing the task‚Äôs completion.\n\nUse Case Diagram:\n\n\n\nNon-functional Requirements\n\n\n\n\n\n\n\nUsability\nThe UI/UX should be modern, elegant, and stylish, designed for easy input and editing of tasks and events. Users should be able to easily distinguish between the calendar view and the todo list view. Adding tasks from the todo list to the calendar should not cause confusion but should remain neat and orderly. Users should be able to input and schedule tasks with minimal effort.\n\n\n\n\nSecurity\nUser data should be protected during transmission and at rest using modern encryption protocols, especially for the enterprise version and all long-range versions, ensuring the security of personal and enterprise data without leaks and protecting user privacy.\n\n\nReliability\nTo ensure data integrity, in addition to the sync and cloud data storage features available in the subscribed personal and enterprise versions, the accuracy and retrieval of data saved on local devices should not be compromised.\n\n\nFlexibility\nUsers should have the capability to customize the theme by specifying RGB values, enhancing personalization and improving accessibility for individuals with color vision deficiencies. The platform offers a configurable setting that allows users to select their preferred calendar layout, be it weekly or daily. Additionally, the default duration assigned to new tasks is fully adjustable, ensuring users can tailor their scheduling to fit their unique requirements.\n\n\nMaintainability\nThere needs to be long-term maintenance of data information.Ensure that updates to the application can be rolled out without significant downtime or data migration issues.\n\n\nPerformance\nThe application needs to be responsive with quick response times, especially since it combines a calendar and to-do list, which are sensitive to time. The software needs to manage the relationship between overdue tasks and future tasks, which requires the software to focus on updating tasks.\n\n\n\n\n\nOther Requirements\nClient Engagement:\nKeep the client involved throughout the development process for feedback and iterations on the product design and features.\nBudgets:\nThe total budget offered by DoMore Software Corp.¬†is two million dollars. There is a million dollars for the mobile version of the application and another million upon completion of the desktop application.\nSubscription:\nThe personal use version is completely free for local devices, but does not include any synchronization updates or storage. Subscriptions are voluntary and require the customer‚Äôs consent. The business version needs to communicate with DoMore Software Corp.¬†and is a paid service.\nAppearance:\nThe UI is expected to be modern, elegant and sleek appearance to attract the users. Users can choose dark mode or light mode with customizable color themes., but there is currently no option for theme selection. It is available for users to choose different colors for tasks by default.\n\n\nGlossary\n\n\n\n\n\n\n\nTask\nAn entity for the todo list feature and will be assigned to different durations.\n\n\n\n\nSub-task\nSeveral breakdown parts for a major task\n\n\nEvent\nAn entity for the calendar feature. Scheduled activities that occur at a specific time, such as classes or meetings, as opposed to tasks.\n\n\nTag\nKeywords or phrases that can be attached to tasks to categorize them and facilitate filtering.\n\n\nTime Window Filter\nFeatures in Dolendar that allow users to set specific time frames during which certain tagged tasks should be focused on.\n\n\nSyncing\nUsers‚Äô data are stored on a cloud server, enabling access across multiple devices and collaboration with others.\n\n\nUser Interface (UI)\nThe interaction between the user and a digital device or software application, includes all the visual elements.\n\n\n\n\n\nAssumptions / Risks\nThere is no server cost for the free personal use version. The paid version gives users‚Äô syncing capabilities, DoMore Software hosts the cloud syncing service. Assuming the user has a basic understanding of to-do lists and digital calendars and is seeking productivity enhancements and solutions for procrastination through the application.\nThe risks associated with Dolendar include potential financial risks due to market competition and whether the budget is sufficient. There are many calendar or to-do list applications on the market, including Google and Apple, so there is significant competition. Additionally, there is a risk associated with the users, who may not fully engage with the task management system. The most important risk stems from data, requiring robust security measures to prevent leaks. Dolendar involves personal privacy data and may contain confidential or proprietary documents from different companies.\n\n\nPriorities / Implementation Phases\nTimeline:\n\n\n\n\n\n\n\nNovember 3rd, 2023\nSubmission of the preliminary Dolendar System Requirements Specification document, excluding functional requirements and use cases.\n\n\n\n\nNovember 17th, 2023\nCompletion of the Dolendar System Requirements Specification, inclusive of functional requirements and use cases.\n\n\nJanuary 7th, 2024\nLaunch of the initial prototype of Dolendar, with subsequent versions to be released through monthly iterations.\n\n\nNovember, 2024\nThe release of the Dolendar mobile application.\n\n\nApril, 2025\nThe debut of the Dolendar desktop application.\n\n\n\nImplementation:\nMr.¬†Dones emphasized that the prototype should have a new version updated monthly after the initial launch, and he also hopes to be involved in the design of the UI. The priority is to spend one year developing the application for iOS and Android systems on both mobile phones and tablets. Then, use the following six months to transition the mobile version to support desktop environments, including macOS, Windows, and Linux, allowing Dolendar to be used on desktop platforms as well.\n\n\nFuture Directions and Expected Changes\nThe expected changes are to enhance team functionalities for task sharing, collaboration, and management. Beyond the current team features, more discussable and joinable functions will be endowed in the enterprise and individual subscription versions.\nDolendar‚Äôs future direction is to update and refine some details of the software continuously. In the future, it may expand to provide users with more data for reference regarding their productivity and efficiency, as well as the extent of their procrastination. The metrics include graphs, rankings, and other metrics, as well as displays of each task‚Äôs completion status and more advanced reminders. The Dolendar platform anticipates future expansion and integration, supporting more systems and versions, as well as creating desktop quick-tips like Apple‚Äôs widgets. It also aims to support more external calendar synchronization, including Microsoft Outlook, but currently, it is only Apple Calendar and Google Calendar. Moreover, an essential part of the future direction is to consider supporting and aiding more people with disabilities in advancing accessibility."
  },
  {
    "objectID": "media/Usability-Evaluation.html",
    "href": "media/Usability-Evaluation.html",
    "title": "Ruilin Wu",
    "section": "",
    "text": "Usability Evaluation\nThis report is based on the interactive prototype created for Sketching and Prototyping Report, which redesigned the ZotFinder campus navigation app to better support commuter students‚Äô safety, comfort, and route-planning needs. In this updated prototype, users can choose between different route options (such as walking-only versus walk-plus-bus) and see how the overall trip experience would look, including time and segments. The design introduces features such as safety and comfort filters (e.g., ‚ÄúAvoid Park,‚Äù ‚ÄúWell-lit Route,‚Äù ‚ÄúShaded Areas‚Äù), bus crowdedness indicators, and an optional AR view for orientation, so that users can gain an approximate understanding of how the full journey would feel even though not all features are fully implemented. In this report, I describe the usability evaluation of that prototype, including the evaluation script, the main task given to participants, a summary of their qualitative feedback, the key usability issues identified, and proposed changes to improve the interface.\n\nScript\n\nIntro / Greeting\nHi, thank you for taking the time to participate in this usability study.\nMy name is Ruilin, and I‚Äôm a student in INF 131. I‚Äôm doing a short usability study for this class.\n(Optional small talk ): ‚ÄúHow‚Äôs your day going?‚Äù / ‚ÄúHow‚Äôs week 10 treating you?‚Äù\nYou were invited because your experience as a UCI student can help improve a campus navigation app. Today, you‚Äôll be trying out a prototype of a redesigned ZotFinder interface. ZotFinder is a UCI campus map app. This prototype focuses on helping commuter students plan safe and comfortable routes to campus by combining walking, bus information, and route filters.\nThe session will have two parts: First, a short mini-interview about how you usually get to campus and use navigation apps.\nThen, I‚Äôll ask you to perform a few actions using the ZotFinder prototype.\nThe prototype may look like a real app, but because it‚Äôs just a prototype, not everything works. So I‚Äôll give you a specific scenario and guide you through it.\nI want to emphasize that I‚Äôm not testing you, and I‚Äôm testing the design. There are no right or wrong answers. If something is confusing, that means the interface needs improvement. As you work, please think aloud: tell me what you‚Äôre looking at, what you‚Äôre trying to do, and what you expect to happen.\nI‚Äôll be taking notes, but I won‚Äôt include your name in the report. The session should take about 15‚Äì20 minutes, and you can stop at any time.\nDo you have any questions, and are you comfortable continuing?\n\nMini-Interview Questions\n\nHow do you usually get to campus (e.g., walking, bus, driving, biking)?\n\nHow often do you use map or transit apps (Google Maps, Apple Maps, TransLoc, ZotFinder, etc.)?\n\nWhen planning your route, what matters most to you (speed, safety, shade, avoiding crowds, etc.)?\n\nHave you ever felt unsafe or uncomfortable on your way to or around campus? What happened?\n\n\nTask with a prototype / product\nTask 1: Route customization (safety & environment)\n\nPlan a route from the location to Howard Schneiderman Lecture Hall that best fits your preferences for safety and comfort (for example, avoiding parks at night or choosing shaded/quiet paths).\n\nHow can you use the filters (e.g., ‚ÄúAvoid Park,‚Äù ‚ÄúWell-lit,‚Äù ‚ÄúShaded,‚Äù ‚ÄúQuiet Route‚Äù) or the Day/Night toggle to adjust the route to your preferences?\n\nFollow-up / backup prompts\n\nWhat will you do next?\nWhat did you expect to happen when you enabled or disabled that filter?\nWhy did you choose this combination of filters instead of another one?\n\nTask 2: Transit choice (real-time bus decision)\nUsing the same trip to Howard Schneiderman Lecture Hall, choose how you would get there if you decide to take the bus instead of walking the whole way.\n\nUse the bus section in the prototype to help you decide. In particular, look at the bus arrival times and the stick-figure crowding icons (for example, two versus four figures) that represent the real-time bus capacity, and decide whether you would take the next bus or wait for another one.\n\nFollow-up / backup prompts\n\nWhat did you expect to see when you tapped on that bus option?\n\nHow are you interpreting the bus icons, the number of ‚Äúlittle people,‚Äù and the times when you decide which bus to take?\n\nHow are you using the real-time bus capacity information to tell whether the bus is really crowded or not?\n\nWhy did you choose this bus instead of the other option?\n\nScripts: Imagine you are a commuter student living off campus and heading to an afternoon class at Howard Schneiderman Lecture Hall. Using this prototype, please plan a route from your current location to Howard Schneiderman Lecture Hall that best fits your preferences for safety and comfort.\nPost-Task Interview Questions\nThese are based on the qualitative prompts in the course resource (task + overall feedback).\nTask-related feedback:\n\n\n\nWhat went well when you were planning your route? What felt clear?\n\nWhat was confusing or difficult while you were doing the task?\n\nWere there any moments you felt stuck or unsure what to do next?\nOverall interface feedback\nOverall, what did you like about this interface?\n\nWhat parts of the interface felt visually clear or easy to understand?\n\nWhat did you dislike or find frustrating?\n\nWere any labels, icons, or buttons unclear or ambiguous?\n\nIs there anything you wish this interface could show that it currently doesn‚Äôt?\n\n\n\nOutro\nThat‚Äôs all the tasks I have for today. Thank you so much for your time and feedback.\nYour comments about what was confusing or helpful will inform the next version of this design. I‚Äôll use what you shared to improve how ZotFinder supports route planning and safety preferences.\nIf you think of anything else later, feel free to let me know. Thanks again, and have a great day!\n\n\n\nTask Summary\nFor the main tasks, both participants were able to complete what the prototype asked them to do. They could find and switch between the two route options, one that involved walking only and one that combined walking and the bus, and they were able to choose a route that matched their needs. Then picked the ‚ÄúGO‚Äù button to see how the interface would look accordingly and ‚ÄúEND‚Äù when they wanted to exit. This shows that the basic task flow of noticing that the app offers more than one route, comparing them, and selecting one was clear enough in the interface. Both participants also understood that the filters were intended to shape the route. They could turn options such as ‚ÄúAvoid Park,‚Äù ‚ÄúWell lit,‚Äù and ‚ÄúShaded‚Äù on and off, and they saw that these settings were related to safety and comfort. At a task level, the core elements required to plan and adjust a route were discoverable and allowed both users to reach a reasonable solution.\nAt the same time, several aspects of the tasks did not feel smooth. For both participants, the filters section required extra time and close reading before they felt confident about which filters were turned on and which were off. They could manipulate the controls, but it was not immediately obvious from a quick glance which conditions were currently applied to the route. This slowed down their decision making. The bus information also led to uneven understanding between the two participants. The first participant, who had experience with ZotFinder and with TransLoc, could interpret the bus times and crowd icons well enough to conclude that the first bus looked too full. The second participant, who rarely uses ZotFinder, had to guess the meaning of the bus numbers and the small person icons and was not sure how real time or reliable this data was. Because of this, he could not fully trust the bus information when performing the task. Overall, the interface supported basic route selection and customization, but the clarity of filter states and the meaning of the bus details needs improvement so that the tasks feel more immediate and intuitive for users with different levels of experience.\nFrom an overall interface perspective, both participants commented that the general layout made sense to them. They could see route options in one area, filters grouped in another area, and bus information in a separate section, and this structure helped them understand what part of the screen to look at for each type of decision. However, both also noted that, because this was their first exposure to this particular redesign, it took them time to interpret the meaning of specific controls and icons. This suggests that while the high level organization of the interface is clear, the finer details of how information is presented and labeled are not yet as self explanatory as they could be.\n\n\nParticipation Feedback\nBoth participants were fourth year UCI undergraduate students who live off campus and commute regularly, but they came to the evaluation with different levels of experience using ZotFinder and transit apps. The first participant was an experienced ZotFinder user and had also used TransLoc before. She quickly understood the basic structure of the redesigned interface and moved through the tasks with relatively little hesitation. When asked to plan a route, she chose to walk to Howard Schneiderman Lecture Hall and used the filters to remove routes she did not want. She described the filters as a way to ‚Äúfilter out the routes I do not want,‚Äù and she was able to apply conditions such as avoiding certain areas and then select the remaining route that fit her schedule and comfort level. She relied on the time and distance information on the route cards to understand the overall trip duration and liked that she could compare options without doing extra calculations. In the bus section, she used the times and crowd icons to judge that the first bus looked quite full and that she did not have enough time to wait for a later one, so she decided to walk instead. She said she liked having bus information in the interface and found the small person icons clear and visually appealing. At the same time, she commented that the Day and Night toggle felt unnecessary because preferences like wanting a well lit or quiet route were already covered by other filters, which made that toggle feel somewhat redundant.\nThe second participant had barely used ZotFinder before and behaved more like a novice encountering the system for the first time. He also successfully completed the tasks, choosing the combined walk and bus route, but he spent a longer time simply looking at the screen and trying to interpret what each section and control meant. He eventually realized that the filters and buttons were interactive, yet he did not quickly understand what each filter would change or how the route options, filters, and bus information were connected. The bus section was especially challenging for him. Unlike the first participant, who could draw on previous experience with TransLoc to interpret the crowd icons, he said he was not sure what the bus numbers and small person icons were meant to show about real time capacity. He needed extra time to guess the meaning behind these visuals and did not feel very confident in using them to decide whether to board the next bus or wait. Together, their feedback suggests that the design works fairly well for experienced users who already understand transit style displays, but it does not yet offer enough built in guidance or clarity for users who are new to ZotFinder or to bus capacity visualizations.\n\n\nUsability Issues and Severity\nFor this evaluation, I used a four-level severity scale:\n1 = Cosmetic, 2 = Minor, 3 = Major, 4 = Critical.\nNo critical issues (4) were observed; most fell between minor and major.\nIssue A: Interface assumes prior familiarity with ZotFinder\n\nSeverity: 2 (Minor)\n\nDescription: The interface seems to assume that users already have some familiarity with ZotFinder or similar navigation tools. Participant 1, who had both ZotFinder and TransLoc experience, navigated quickly and understood the basic structure. Participant 2, who rarely uses ZotFinder, spent more time just looking at the screen and was less sure where to start or what each section was for. Both participants were able to complete the tasks, but for less experienced users the first interaction requires more effort to understand the layout and available controls.\n\nIssue B: Uneven understanding of bus information and real-time capacity\n\nSeverity: 2‚Äì3 (Minor to Major)\n\nDescription: Participant 1 understood the bus information fairly well, in part because she was already used to reading similar information in TransLoc. She could interpret the stick-figure icons as crowd indicators and use them to decide that the first bus looked too full. In contrast, Participant 2 was not confident about what the bus numbers and ‚Äúlittle business man‚Äù icons meant and had to guess the meaning behind them. He was not sure how real-time the data was or how strongly he should rely on it. This uneven understanding suggests that the current design communicates bus information clearly to some experienced users, but does not provide enough explicit support for users who are less familiar with transit apps.\n\nIssue C: Time needed to interpret filters and customization controls\n\nSeverity: 2 (Minor)\n\nDescription: Both participants mentioned that, because this was their first time using this particular redesign, it took them some time to understand what each filter and control meant. Participant 1 eventually used the filters comfortably and described them as a way to ‚Äúfilter out the routes I don‚Äôt want,‚Äù while Participant 2 needed longer to interpret what each option would do. This is not a severe problem, but some orientation time is expected with a new interface, which indicates that the meaning of individual filters and the relationship between filters, route options, and bus information could be made clearer to reduce the initial interpretation effort.\n\nIssue D: Filter state visibility and redundancy of the Day/Night toggle\n\nSeverity: 2 (Minor)\n\nDescription: Both participants could toggle filters on and off, but they had to look carefully to see which ones were currently enabled. The visual difference between active and inactive filters is not strong enough to be read at a glance. In addition, Participant 1 commented that the Day/Night toggle felt unnecessary because similar preferences (for example, ‚ÄúWell-lit‚Äù or ‚ÄúQuiet Route‚Äù) were already available as filters. This points to a minor visibility issue (which options are active) and a minor redundancy issue (two ways to express similar preferences), which can cause extra thinking without blocking task completion.\n\n\n\nRecommended Interface Changes\nThe following changes are directly tied to the issues above and are designed to make the interface easier to interpret on a first use, while still supporting experienced ZotFinder users.\nChange 1: Strengthen filter state visibility and clarify roles of filters vs.¬†Day/Night\nThis change addresses Issue C and Issue D.\nThe Filters section should more clearly indicate which options are active. Active filters could appear as filled pills with bold text and a small checkmark, while inactive filters remain outlined and lighter. This would allow users to quickly scan and see which conditions are shaping the route, instead of having to inspect each label. In parallel, the role of the Day/Night toggle should be clarified or simplified. One option is to remove it and rely entirely on the filters for safety and comfort preferences. Another option is to make it a preset that turns on relevant filters (for example, ‚ÄúNight‚Äù automatically enabling ‚ÄúWell-lit‚Äù and ‚ÄúAvoid Park‚Äù) and briefly explain this behavior. These changes are adequate because they directly reduce the time users spend interpreting controls, make the active state of the route more transparent, and avoid overlapping controls that may confuse users about what actually changes the route.\nChange 2: Add explicit explanations for bus information and real-time capacity\nThis change addresses Issue B.\nTo support users like Participant 2, who do not come in with prior TransLoc or transit-app experience, the bus information should explain its meaning more explicitly. A short label or legend could clarify that the stick-figure icons represent real-time or near real-time bus capacity and that more figures indicate a more crowded bus. A brief phrase near the times, such as ‚ÄúLive arrival time‚Äù or ‚ÄúEstimated arrival,‚Äù would clarify how current the timing is. This change is adequate because it removes the need for users to guess what the icons and times mean, brings users with less experience up to the same understanding that Participant 1 already had, and directly supports the task of deciding whether to board the next bus or wait for another.\nChange 3: Provide light, first-use guidance for the overall layout\nThis change addresses Issue A and supports Issue C.\nTo reduce the assumption of prior familiarity, the interface can include very lightweight guidance for first-time use. For example, a short inline hint near the Filters section, such as ‚ÄúUse filters to customize your route based on safety and comfort‚Äù, and a small label near the routes like ‚ÄúCompare route options‚Äù would help users quickly understand what each area is for without requiring a full tutorial. Because both participants mentioned that it took them some time to interpret the controls on first use, these small, targeted hints are adequate: they reduce initial confusion, shorten the learning curve, and still keep the interface clean for repeat use.\nTogether, these changes respond directly to the issues observed in the sessions. They focus on making states and meanings more legible, rather than adding new features, so they are well aligned with the kinds of confusion participants actually experienced during the evaluation."
  },
  {
    "objectID": "media/Breast_Cancer_Recurrence_Prediction.html",
    "href": "media/Breast_Cancer_Recurrence_Prediction.html",
    "title": "Introduction",
    "section": "",
    "text": "Final Project: Breast Cancer Recurrence Prediction\nShuyi Chen, Ruilin Wu\n\nIntroduction\nBreast cancer is one of the most common malignancies affecting women worldwide, and its recurrence remains a serious concern, often occurring months or years after initial breast cancer treatment. Various machine learning models have been explored for predicting breast cancer recurrence, from Naive Bayes classifiers to complex deep learning approaches. While advanced models like CNNs and NLP-based systems show high accuracy, they often depend on unstructured data and complex infrastructure. In contrast, Kim et al.¬†(2016) demonstrated that a Naive Bayesian model can effectively predict five-year recurrence, highlighting its clinical potential.\nThis report focuses on applying Bayesian Logistic Regression‚Äîan interpretable and widely used method‚Äîto evaluate its effectiveness in predicting breast cancer recurrence using a structured clinical dataset, aiming to support transparent and reliable clinical decision-making.\n\n\nData Description\nThe breast cancer dataset was obtained from the University Medical Centre, Institute of Oncology, located in Ljubljana, Yugoslavia. The dataset consists of 286 instances categorized into two distinct outcomes: 201 instances represent cases with no recurrence events, while 85 instances indicate cases with recurrence events. It is important to note that the dataset contains missing values, necessitating proper handling in subsequent analytical and modeling procedures.\nEach instance within the dataset is described by nine attributes, which include both linear and categorical variables. The attributes include age (categorized in decade ranges from 10 to 99 years), menopause status (categorized as less than 40 years, greater or equal to 40 years, or premenopausal), tumor size (categorized in increments of five millimeters, ranging from 0 to 59 mm), the number of involved axillary lymph nodes (categorized in increments of three, ranging from 0 to 39), presence or absence of capsular invasion (binary classification of yes or no), degree of malignancy (ordinal scale from 1 to 3, indicating increasing severity), the affected breast side (left or right), tumor location within breast quadrants (left-upper, left-lower, right-upper, right-lower, or central), and the administration of irradiation therapy (binary classification of yes or no).\n\n\nExploratory Data Analysis\nAfter removing rows with missing values, our dataset contained 277 observations. We also excluded two single-row categories‚Äîone instance of invnodes ‚Äú24-26‚Äù and one instance of age ‚Äú20-29‚Äù‚Äîsince they each occurred only once and could cause modeling or cross-validation issues. This left us with a final total of 275 observations. The dataset shows a moderate imbalance in the outcome variable, with roughly 71% of cases being no-recurrence events and 29% recurrence events (Figure 1). Most patients fall within the 30‚Äì69 age range, and the menopausal status is split mainly between ‚Äúpremenopause‚Äù and ‚Äúgreater or equal to 40,‚Äù while the ‚Äúless than 40‚Äù group remains sparse. The cross-tabulation (Table 1) indicates that most patients in both outcome groups are between 30 and 69 years old, with the highest counts in the 40‚Äì59 age ranges.\n\n\nMethodology\nWe fit a Bayesian logistic regression model (using stan_glm() in the rstanarm package) to predict breast cancer recurrence, employing the binomial family with a logit link. Each predictor‚Äîage, menopause, tumorsize, invnodes, nodecaps, degmalig, breast, breastquad, and irradiat‚Äîwas included as a fixed effect. We applied weakly informative normal priors (Œº = 0, œÉ = 2.5) for coefficients and (Œº = 0, œÉ = 5) for the intercept, without imposing overly strong assumptions on effect sizes.\nFour Markov Chain Monte Carlo (MCMC) chains were run for 10,000 iterations each, and we evaluated convergence via the Gelman‚ÄìRubin statistic (R-hat) and effective sample size. The posterior predictive check (Figure 2) suggested that the model adequately captured the distribution of observed outcomes, supporting its overall fit to the data. Furthermore, the MCMC density plot (Figure 3) demonstrated well-behaved sampling behavior across all parameters, with consistent overlap between chains and unimodal posterior distributions. After confirming convergence, we extracted 80% credible intervals for the model‚Äôs coefficients (and exponentiated these to interpret them as odds ratios). Model performance was assessed by computing predicted probabilities for each observation, then constructing a ROC curve and identifying an optimal classification threshold using Youden‚Äôs index (Figure 4). Finally, a 10-fold cross-validation (classification_summary_cv()) was performed at the chosen threshold (0.33) to estimate sensitivity, specificity, and overall accuracy in an out-of-sample context.\n\n\nResults\nAll chains converged satisfactorily, with R-hat and sufficiently large effective sample sizes. The 80% credible intervals for the exponentiated coefficients reveal that invnodes3‚Äì5, invnodes6‚Äì8, invnodes9‚Äì11, and degmalig lie entirely above 1, indicating a higher probability of recurrence compared to their reference categories. In contrast, tumorsize10‚Äì14 sits entirely below 1, suggesting lower recurrence odds than the reference tumor‚Äêsize bin. Most other predictors, such as age brackets, menopausal status, most tumor‚Äêsize categories, have intervals crossing 1, meaning the model cannot rule out either increased or decreased risk given the data‚Äîthese effects remain uncertain. Finally, the intercept‚Äôs exponentiated interval falling far below 1 indicates that, when all predictors are at their reference levels, the baseline odds of recurrence are quite low; other predictors then raise or lower this low baseline as indicated by their respective odds ratios.\nA ROC curve analysis on the entire dataset suggested a 0.33 probability threshold for optimal classification (Figure 4). Applying this threshold in the 10-fold cross-validation produced a sensitivity of roughly 67.1%, specificity of about 71.3%, and an overall accuracy near 69.5%.\nConclusion\nIn the context of this breast cancer dataset, the Bayesian logistic regression model provided a moderate level of predictive power, aligning with expectations for a relatively small clinical dataset. The model‚Äôs interpretability is its key advantage: practitioners can directly review posterior intervals and odds ratios to understand how clinical features‚Äîparticularly tumor size, lymph node involvement, and degree of malignancy‚Äîinfluence breast cancer recurrence risk. While most variables had limited impact or wider credible intervals due to sparse data, the approach still offered transparent risk stratification. Future work might include incorporating additional patient features such as genetic markers or exploring more robust priors to potentially enhance performance.\n\n\nReferences\nKim, Woojae, et al.¬†‚ÄúNomogram of Naive Bayesian Model for Recurrence Prediction of Breast Cancer.‚Äù Healthcare Informatics Research, vol.¬†22, no. 2, 30 Apr.¬†2016, p.¬†89, https://doi.org/10.4258/hir.2016.22.2.89.\nWang, Hanyin, et al.¬†‚ÄúPrediction of Breast Cancer Distant Recurrence Using Natural Language Processing and Knowledge-Guided Convolutional Neural Network.‚Äù Artificial Intelligence in Medicine, vol.¬†110, 1 Nov.¬†2020, p.¬†101977, https://doi.org/10.1016/j.artmed.2020.101977.\n\n\nAppendix\n\nFigure 1. Distribution of Recurrence Events\n\n\nFigure 2. PP Check Plot\n\nFigure 3. MCMC Density Plot\n\nFigure 4. ROC Curve"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ruilin Wu",
    "section": "",
    "text": "LinkedIn\n  \n  \n    \n     GitHub\n  \n  \n    \n     Email\n  \n\n  \n  \n\n\n\nHello!üëã I‚Äôm a fourth-year Data Science major (Informatics minor) at the University of California, Irvineüéì. My work focuses on learning analytics, AI-supported writing, and equitable EdTech: I analyze multi-quarter course datasets and translate findings into clear, actionable insights for instructors and researchers, with an emphasis on transparency, reproducibility, and equity.\nAt UCI‚Äôs Digital Learning Lab, I‚Äôve been involved in both Converse to Learn project and Generative AI projectü§ñ. I‚Äôm also a CP-LEADS Fellow and a PALiISaDS Scholar, experiences that strengthen my training in statistics, machine learning, and research communication. Outside academia, I serve as an HR Board Member for the Chinese Union at UCI, supporting event analytics, operations, and student engagementü§ù.\nMy interests include Machine Learning, Educational Data Science, NLP for Writing Support, Statistical Modeling, and Data Visualizationüìä ‚Äî especially building transparent, scalable, and human-centered data systems that turn real-world data into meaningful impactüî¨.\n\n\n\n\nRPythonSQLJavaFigma"
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Ruilin Wu",
    "section": "",
    "text": "Hello!üëã I‚Äôm a fourth-year Data Science major (Informatics minor) at the University of California, Irvineüéì. My work focuses on learning analytics, AI-supported writing, and equitable EdTech: I analyze multi-quarter course datasets and translate findings into clear, actionable insights for instructors and researchers, with an emphasis on transparency, reproducibility, and equity.\nAt UCI‚Äôs Digital Learning Lab, I‚Äôve been involved in both Converse to Learn project and Generative AI projectü§ñ. I‚Äôm also a CP-LEADS Fellow and a PALiISaDS Scholar, experiences that strengthen my training in statistics, machine learning, and research communication. Outside academia, I serve as an HR Board Member for the Chinese Union at UCI, supporting event analytics, operations, and student engagementü§ù.\nMy interests include Machine Learning, Educational Data Science, NLP for Writing Support, Statistical Modeling, and Data Visualizationüìä ‚Äî especially building transparent, scalable, and human-centered data systems that turn real-world data into meaningful impactüî¨."
  },
  {
    "objectID": "index.html#skills",
    "href": "index.html#skills",
    "title": "Ruilin Wu",
    "section": "",
    "text": "RPythonSQLJavaFigma"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Poster Presentations\nAAAS S-STEM Scholars Meeting ‚Äî San Diego, CA\nDates: October 2‚Äì4, 2025 ¬∑ Theme: Social Science\nTitle: Examining the Impact of an AI-Powered Writing Platform in Upper-Division Engineering Courses\nView poster\nExpanded analysis across multi-quarter:\n\nTotal AI interactions are a modest predictor of outcomes.\n\nInstructional context and module design explain more variance.\n\nPlanning and revision modules are consistent student needs.\n\n\nUCI Undergraduate Research Symposium ‚Äî Poster Finalist ‚Äî Irvine, CA\nDate: June 16, 2025 ¬∑ Theme: Human Experience and Expression\nTitle: Examining the Impact of an AI-Powered Writing Platform in Upper-Division Engineering Courses\nView poster\nInitial findings (Spring 2024 cohort):\n\nHigher-performing students engaged more frequently and wrote more concisely.\n\nLower-performing students engaged less often but provided longer, more elaborate inputs per conversation.\n\n\n\n\nResearch Experience\nResearch Assistant, UCI Digital Learning Lab ‚Äî Summer 2024‚ÄìPresent\nAdvisor: Dr.¬†Mark Warschauer\n\nPapyrusAI (Jan 2025‚ÄìPresent):\nData cleaning, exploratory analysis, and usage-pattern evaluation for an AI writing support system used across multiple course offerings and student populations.\nConverse to Learn (Jun 2024‚Äì Aug 2025):\nData collection and annotation for a study on how conversational agents shape student engagement and learning.\n\nImpact: Strengthened skills in educational data science, human‚ÄìAI interaction, and quantitative research; collaborated with interdisciplinary teams to translate analyses into actionable insights for instruction and tool design."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "ZotFinder Campus Navigation: UX Research & Interface Redesign\nOverview: A research-driven redesign of UCI‚Äôs ZotFinder app to better support off-campus commuters‚Äîfrom planning safe routes to finding quiet, comfortable study spaces on campus.\nWhat I did:\n\nInterviewed and observed students to understand pain points in commuting, wayfinding, and study-space selection.\nCreated a persona and scenario, then designed an interactive mobile prototype in Figma.\nRan think-aloud usability tests with student participants and synthesized feedback into concrete UI and interaction improvements.\n\nüëâ Visit ZotFinder Redesign Interface\nüëâ Contextual Inquiry üëâ Sketching and Prototyping üëâ Usability Evaluation\n\n\n\nAge- and Context-Appropriate AI Chat for Children\nOverview: An interactive AI chatbot tailored to three developmental bands‚Äîages 5‚Äì7, 8‚Äì12, and 13+; built with OpenAI ChatGPT API and Retrieval-Augmented Generation (RAG) for accurate, age-appropriate, context-aware replies.\nHow it works:\n\nCollects age and region at conversation start to customize tone, vocabulary, and examples.\nUses RAG to ground responses in curated knowledge, improving safety and relevance.\nPrioritizes responsible interaction patterns for kids.\n\nüëâ Visit A Little Secret\n\n\n\nCapstone Project ‚Äî Delayed Antibiotics Administration\nPartner: Children‚Äôs Hospital of Orange County (CHOC)\nDuration: Jan 2025 ‚Äì Jun 2025\nGoal: Predict and classify the severity of delayed antibiotic administration to support proactive clinical intervention.\nMy contributions:\n\nData Engineering: Built an ETL pipeline to consolidate multi-source pediatric data; enabled reliable feature engineering and modeling.\nModel Development: Led a Random Forest classifier; hyperparameter tuning, feature analysis, and baselines against alternative models.\nEvaluation & Communication: Assessed with ROC curves and confusion matrices; presented findings and implications to clinical stakeholders.\nCollaboration & Management: Translated clinician requirements, validated outputs, and maintained code/docs in GitHub.\n\nüëâ Project site\n\n\n\nBreast Cancer Recurrence Prediction\nFocus: This project applies Bayesian Logistic Regression to predict five-year breast cancer recurrence using a structured clinical dataset (n = 275). The goal is to build an interpretable, uncertainty-aware modeling workflow that supports clinical decision-making.\nOutcome: The Bayesian logistic regression model identified axillary lymph node involvement (3‚Äì11 nodes) and higher malignancy degree as strong predictors of recurrence, while small tumors (10‚Äì14 mm) were associated with lower risk. Using a ROC-optimized threshold of 0.33, the model achieved ~67% sensitivity, ~71% specificity, and ~70% cross-validated accuracy, providing interpretable, uncertainty-aware risk estimates for breast cancer recurrence.\nüëâ Detailed report üëâ R code\n\n\n\nFactors Related to Computer Science Conference Publications\nQuestion: Which variables are associated with higher conference publication output?\nApproach:\n\nExtensive data cleaning/wrangling/mining on open-source datasets.\nUsed Python to parse and aggregate JSON data from NSF and NIH, aligning funding/activity features with publication counts.\nWrote SQL queries to extract and join relational data across multiple tables.\nBuilt tidy, analysis-ready tables for modeling and comparison.\n\nOutcome: Funding is a weak predictor of publication output for individual researchers, but a strong predictor at the institutional level, where regional differences further influence how effectively funding translates into high-impact computer science publications.\nüëâ Data processing & cleaning\nüëâ Final analysis report\n\n\n\nDolendar System Requirements\nScope: Translated a 2-hour client consultation into a comprehensive, structured requirements document.\nDeliverable: Clear user stories, functional/non-functional specs, constraints, and acceptance criteria to guide development.\nüëâ Full report"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Phone: 5035488226\nEmail: ruiliw5@uci.edu\nInstagram\nWeChat"
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About",
    "section": "",
    "text": "Phone: 5035488226\nEmail: ruiliw5@uci.edu\nInstagram\nWeChat"
  },
  {
    "objectID": "media/Stats170A_a2.html",
    "href": "media/Stats170A_a2.html",
    "title": "Abstract",
    "section": "",
    "text": "Analyzing Factors Related to Conferences Publication\nShuyi Chen, Ruilin Wu\n\nAbstract\n\nThis study examines how National Science Foundation (NSF) and National Institutes of Health (NIH) funding correlates with computer science conference publications from 2020 to 2024. After integrating open-source data with an existing database, we identified 1,840 principal investigators across 213 institutions. At the PI level, funding shows a weak relationship to publication counts (R¬≤ = 0.00174). In contrast, institution-level funding strongly predicts publication productivity (adjusted R¬≤ = 0.733). Incorporating regional interactions further improves explanatory power (adjusted R¬≤ = 0.8068), revealing a lower funding-to-publication ‚Äúreturn‚Äù in the South compared to the Midwest. While the findings underscore funding‚Äôs importance institutionally, causal inferences are limited, and longer timelines may be needed to capture the full effect of recent grants.\n\nIntroduction\n\nConducting research and publishing findings are essential for advancing the field of computer science, and each study requires substantial financial support. Analyzing adjusted publication counts, which consider the number and ranking of authors in each paper, motivates us to examine the funding sources behind these publications. NSF and NIH datasets are processed separately, then merged after extracting principal investigator (PI) names and awarded funding. To maintain consistency, funding amounts for PIs with multiple awards are aggregated. Finally, the organized NSF and NIH datasets are merged with our existing cleaned database regarding the Computer Science field to explore potential correlations between the funding amounts received by principal investigators and their publication counts. This analysis aims to provide a deeper understanding of how research funding influences publication output in the field of computer science.\n\nStatistical Methods\n\nFrom 2020 to 2024, a total of 34,147 principal investigators (PIs) received funding from the National Science Foundation (NSF), while 76,219 PIs were funded by the National Institutes of Health (NIH) across various research fields worldwide.\nOur existing cleaned dataset contains the information of 17,837 principal investigators. After matching these records with the existing cleaned dataset, 1,737 PIs from NSF were identified. Their average total funding was $1,356,554, with a mean total adjusted publication count of 7.31. Similarly, 211 PIs from NIH were matched, receiving an average total funding of $2,788,723, with a mean total adjusted publication count of 5.07. The adjusted publication counts are right-skewed, with most PIs and institutions having low counts, while a few contribute significantly more.\nAfter merging the NSF and NIH awarded principal investigators (PIs), a total of 1,840 distinct PIs from 213 different institutions were identified. Their average total funding was $1,600,410, with a mean total adjusted publication count of 7.01. Total funding (log scale) follows a normal-like distribution, with PIs receiving mid-range amounts and institutions exhibiting greater variability. After filtering only US institutions to explore geographical factors, there are 158 institutions that received NSF and NIH funding in the past five years.\nTo assess the relationship between funding amounts and total adjusted publication counts, we employ a linear regression model for further analysis. Linear regression allows us to determine the extent to which funding influences research output by estimating the strength and direction of the association between funding and publication counts. The comparative analysis is conducted at two levels: the principal investigator(PI) level and the institution level. At the PI level, we examine how an individual researcher‚Äôs total awarded funding relates to their adjusted publication count. At the institution level, we aggregate the funding and the adjusted publication counts across institutions to analyze how research funding affects overall publication counts at a broader scale. Additionally, states of the institutions are being grouped into broader geographic regions: Northeast, Midwest, South, and West.\n\nResults\n\nLinear Regression Model 1: Total Adjusted Publications on Total Funding - Principle Investigator (Left)\nLinear Regression Model 2: Total Adjusted Publications on Total Funding - Institution (Right)\nModel 1 shows that at the PI level total funding is a very weak predictor of the total adjusted publication count. The estimated intercept is 6.863, meaning that with zero funding, an individual is predicted to have about 6.863 adjusted publications. The coefficient for total funding is 9.336e-08, indicating that an increase of $1,000,000 in funding is associated with an increase of roughly 0.093 publications. However, this effect is only marginally significant (p = 0.0737), and the model explains just 0.17% of the variance in publication counts (R-squared = 0.00174). Overall, funding does not appear to be a strong predictor of publication productivity at the PI level.\nModel 2 reveals that higher funding at institutional levels are strongly associated with increased publication productivity. Specifically, for every additional dollar of funding, the model predicts an increase of approximately 4.277e-06 in the total adjusted publication count. In more interpretable terms, an increase of $1,000,000 in funding is associated with an increase of about 4.277 adjusted publications. An adjusted R-squared of 0.733 indicates that roughly 73.3% of the variability in publication productivity is explained by differences in funding levels. Moreover, the F-statistic of 435.4 (p-value &lt; 2.2e-16) demonstrates that the overall model is statistically significant, confirming that funding is a very important predictor of publication output.\nLinear Regression Model 3: Total Adjusted Publications on Funding by Region (Interaction Model)\n\n\nModel 3 introduces an interaction between total funding and region, capturing whether the slope of funding‚Äôs effect on publications differs by region. Two of these interactions (Northeast and West) are not significant, however, region South exhibits a significant negative interaction term (-2.527e-06, p = 0.00015), each additional dollar of funding yields fewer publications compared to the Midwest. With an adjusted R-squared 0.8068, Model 3 explains about 80.7% of the variance in total adjusted publications‚Äîan improvement over a non-interactive model. These results suggest that while funding strongly predicts publication output at the institutional level, its effect can vary by region, with the South showing a notably smaller increase in publications per additional dollar of funding.\n\nDiscussion\n\nThe linear regression analyses reveal contrasting relationships between funding and adjusted publication counts at different levels of aggregation. At the principal investigator level, total funding explains very little of the variance in publication output, suggesting that individual productivity is more strongly influenced by factors such as research team size, institutional resources, collaborative networks, and disciplinary publishing norms. In contrast, institution-level analyses indicate a robust positive association between funding and publication productivity: higher funding generally translates into more adjusted publications. This likely reflects that sufficient financial resources enable improved research infrastructure, additional personnel, and stronger collaborative networks‚Äîfactors that collectively foster a higher volume of publications. When regional factors are incorporated via an interaction of funding and region, the model‚Äôs explanatory power improves. Notably, the slope for the South is significantly lower than that of the baseline region, implying that an additional dollar of funding in the South yields fewer publications compared to the baseline. Meanwhile, the Northeast and West do not differ significantly from the Midwest. Although these regional differences are statistically meaningful, funding itself remains the primary predictor of publication output at the institutional level.\nDespite these insights, the analyses do not establish a causal link: higher funding may lead to more publications, but institutions with a strong track record may also be more competitive in securing grants. Additionally, some awards may not bear fruit immediately due to the long timeline needed for project completion and subsequent publication. Future studies could extend the timeframe, investigate the quality of published research, examine discipline-specific publishing norms, and explore longer-term funding effects to gain a more comprehensive understanding of how funding influences publication outcomes in computer science."
  },
  {
    "objectID": "media/Stats170A_a1.html",
    "href": "media/Stats170A_a1.html",
    "title": "Data Processing",
    "section": "",
    "text": "Shuyi Chen, Ruilin Wu\n\n\nacm_fellows.csv contains 1493 rows and 2 columns, and the column names are name and year. The dataset records the awards from 1994 to 2023. There are no duplicated rows and missing values. However, the following names of the researchers who received the award more than once.\n\nconference_ranking.csv contains 1493 rows and 3 columns, and the column names are Antonym, Name and Rank. Conferences are ranked into 4 levels, A*, A, B, C. There are no duplicate rows and missing values. However, among the A* ranked conferences, three different conferences share the same abbreviation, and the abbreviation ICIS appears three times. As a result, relying solely on abbreviations may lead to ambiguity and potential errors.\n\ncountry-info.csv contains 468 rows and 3 columns, with the column names: institution, region, and countryabbrv. The file has no duplicate rows or missing values. Since U.S. institutions are excluded, the majority of institutions are from Europe, totaling 239 institutions. In contrast, Africa has the fewest institutions, with only 3 entries.\n\ncsrankings.csv contains 30400 rows and 4 columns, with the column names: name, affiliation, homepage and scholarid. The dataset has no duplicate rows or missing values. However, when considering only the scholarid column, 10401 scholar IDs appear more than once.\n\ndata.csv contains 1827 rows and 26 columns. The number of total missing values is 8727. Ten columns have different counts of missing values. These missing values may pose challenges in obtaining a complete understanding of the represented CS schools.\n\ndblp-aliases.csv contains 100987 rows and 2 columns, with column names: alias and name. While there are no missing values, the dataset includes 3 duplicate rows.\n\nfield_conference.csv contains 77 rows and 3 columns with the column names: major, field and conference. The major is consistently Computer Science, but the dataset includes 27 distinct fields spanning various conferences.\n\ngenerated-author-info.csv contains 239169 rows and 6 columns, with column names: name, dept, area, count, adjustedcount and year. Within this large dataset, it has no duplicate rows or missing values. From the trend of adjustedcount over the years, showing a significant increase in research publications after 2000.\n\ngeolocation.csv contains 570 rows and 3 columns, with column names: institution, latitude and longitude. The dataset has no duplicate rows or missing values. The latitude and longitude values span both negative and positive ranges, indicating a global coverage. This suggests that CS rankings vary across different countries worldwide.\n\nturing.csv contains 89 rows and 2 columns, with column names: name and year. The dataset has no duplicate rows or missing values. The award years range from 1966 to 2023. Notably, six researchers received the award in 2002, making it one of the most awarded years.\n\n\n\n\nWe processed all the datasets by reading CSV files, transforming specific columns, and storing them in an SQLite database. One key transformation we performed was splitting names into first_name, middle_name, and last_name for datasets like csrankings.csv and generated-author-info.csv. We achieved this using strsplit(df$name, ‚Äú\\\\s+‚Äù), which separated names based on spaces. The first element was assigned as first_name, while the middle portion was combined into middle_name if present. The last element was usually assigned as last_name, but if the last token was numeric, we used the second-to-last word instead to ensure proper name extraction.\nAdditionally, we enhanced the conference_ranking.csv dataset by categorizing conferences based on their affiliations with ACM, IEEE, or both. Once we completed the transformations, we wrote all datasets into an SQLite database using dbWriteTable(), with queries verifying successful data storage. Lastly for this task, we used dbListTables(con) to confirm that all tables have been created, ensuring a structured and organized database for further analysis.\nconference_ranking.csv (added academic society)\n\ncsrankings.csv (the names were separated into its component first name, middle name and last name values)\n\n\ngenerated-author-info.csv (the names were separated into its component first name, middle name and last name values)\n\nacm-fellows.csv\n\n\n\ncountry-info.csv\n\ndata.csv\n\n\n\n(and 16-26 of 26 columns are not showed for space purpose)\ndblp-aliases.csv\n\nfield_conference.csv\n\ngeolocation.csv\n\nturing.csv\n\n\n\n\n\n\n\n\nThe first 10 names from the missing authors list, sorted alphabetically.\n\ngenerated_author_info\n\nacm_fellows\n\nturing\n\n\n\n\nWe combined the three missing authors lists into a single dataframe with distinct names (824 missing names in total). Then we further splitted the name column into first name, middle name, and last name.\n\nFinally, we inserted name, first_name, middle_name, and last_name from missing authors dataframe into csrankings table, other fields values are by default NULL.\n\n\n\n\n\n\n\n\n\nWe use a two-step approach to determine whether two (or more) records from csrankings table represent the same author. First, we define a function that checks if two records have the same last and first name and compatible middle names‚Äîeither an exact match or one name being an initial matching the first letter of the other. For instance, ‚ÄúJohn P. Smith‚Äù and ‚ÄúJohn Paul Smith‚Äù match if they share the same last name, have the same first name, and either matching or initial-matching middle names; we also consider two records duplicates if one or both are missing a middle name but the first and last names match (e.g., ‚ÄúJohn Smith‚Äù and ‚ÄúJohn Paul Smith‚Äù). Second, once we identify groups of authors who are duplicates, we pick one ‚Äúmost complete‚Äù name‚Äîtypically the one with the longest middle name‚Äîto serve as the canonical reference (the ‚Äúsynonym‚Äù) for everyone in that group. This ensures, for example, that ‚ÄúJohn Smith,‚Äù ‚ÄúJohn P Smith,‚Äù and ‚ÄúJohn Paul Smith‚Äù all unify under ‚ÄúJohn Paul Smith.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe following is the representative subsets of the duplicate_author table.\n\n\nThere are a total of 5705 rows of duplicate names being found based on our implementation.\n\n\n\n\nWe retrieved the csrankings and duplicate_author tables from the database, then joined them to replace any duplicate names with their canonical synonyms, standardizing the ‚Äúname‚Äù column. It groups the updated records by this canonical name and merges duplicate rows by selecting, for each attribute, the longest (most complete) value, then writes the resulting clean table back to the database. The cleaned cs_rankings have 28115 rows, compared with cs_rankings 31224 rows, we found and merged 3,109 duplicate rows.\n\n\n\n\nWe update the generated_author_info table by replacing duplicate author names with their canonical synonyms as identified in the duplicate_author table.\nWe deliberately avoid merging rows because we want to preserve the original structure where each row represents a single count entry (e.g., one paper). Merging records would aggregate counts and potentially lose the granularity of the data. By simply replacing the names, we standardize author identifiers across tables while retaining the original row-level detail for further analysis. Therefore, the generated_clean retains 239169 rows.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask 4.1\nSQL Query:\nSELECT\ng.name AS author,\ng.dept AS institution,\nSUM(g.count) AS a_star_publication\nFROM generated_author_info_clean AS g\nJOIN conference_ranking AS c\nON LOWER(g.area) = LOWER(c.Antonym)\nWHERE c.Rank = ‚ÄòA*‚Äô\nAND g.year BETWEEN 2010 AND 2024\nGROUP BY g.name, g.dept\nORDER BY a_star_publication DESC, author ASC, institution ASC\nLIMIT 100;\nOutput:\nFrom the above SQL statement, we have generated top 100 authors. The following screenshot displays the top 10 of them.\n\n\n\n\n\n\n\n\n\nSQL Query:\nWITH RankedInstitutions AS (\nSELECT\nCOALESCE(fc.field, ‚ÄòUnknown Field‚Äô) AS cs_field,\ng.dept AS institution_name,\nCOUNT(*) AS a_star_publications,\nRANK() OVER (\nPARTITION BY COALESCE(fc.field, ‚ÄòUnknown Field‚Äô)\nORDER BY COUNT(*) DESC, g.dept ASC\n) AS rank_position\nFROM generated_author_info_clean g\nJOIN conference_ranking c\nON LOWER(g.area) = LOWER(c.Antonym)\nLEFT JOIN field_conference fc\nON LOWER(g.area) LIKE ‚Äò%‚Äô || LOWER(fc.conference) || ‚Äò%‚Äô ‚Äì Substring matching\nWHERE c.Rank = ‚ÄòA*‚Äô\nGROUP BY COALESCE(fc.field, ‚ÄòUnknown Field‚Äô), g.dept\n)\nSELECT\ncs_field,\ninstitution_name,\na_star_publications\nFROM RankedInstitutions\nWHERE rank_position &lt;= 10\nORDER BY cs_field ASC, a_star_publications DESC, institution_name ASC;\nOutput: From the above SQL statement, we have generated top 10 institutions in each field. The total number of fields is 17."
  },
  {
    "objectID": "media/Stats170A_a1.html#task-1-data-exploration-and-discovery",
    "href": "media/Stats170A_a1.html#task-1-data-exploration-and-discovery",
    "title": "Data Processing",
    "section": "",
    "text": "acm_fellows.csv contains 1493 rows and 2 columns, and the column names are name and year. The dataset records the awards from 1994 to 2023. There are no duplicated rows and missing values. However, the following names of the researchers who received the award more than once.\n\nconference_ranking.csv contains 1493 rows and 3 columns, and the column names are Antonym, Name and Rank. Conferences are ranked into 4 levels, A*, A, B, C. There are no duplicate rows and missing values. However, among the A* ranked conferences, three different conferences share the same abbreviation, and the abbreviation ICIS appears three times. As a result, relying solely on abbreviations may lead to ambiguity and potential errors.\n\ncountry-info.csv contains 468 rows and 3 columns, with the column names: institution, region, and countryabbrv. The file has no duplicate rows or missing values. Since U.S. institutions are excluded, the majority of institutions are from Europe, totaling 239 institutions. In contrast, Africa has the fewest institutions, with only 3 entries.\n\ncsrankings.csv contains 30400 rows and 4 columns, with the column names: name, affiliation, homepage and scholarid. The dataset has no duplicate rows or missing values. However, when considering only the scholarid column, 10401 scholar IDs appear more than once.\n\ndata.csv contains 1827 rows and 26 columns. The number of total missing values is 8727. Ten columns have different counts of missing values. These missing values may pose challenges in obtaining a complete understanding of the represented CS schools.\n\ndblp-aliases.csv contains 100987 rows and 2 columns, with column names: alias and name. While there are no missing values, the dataset includes 3 duplicate rows.\n\nfield_conference.csv contains 77 rows and 3 columns with the column names: major, field and conference. The major is consistently Computer Science, but the dataset includes 27 distinct fields spanning various conferences.\n\ngenerated-author-info.csv contains 239169 rows and 6 columns, with column names: name, dept, area, count, adjustedcount and year. Within this large dataset, it has no duplicate rows or missing values. From the trend of adjustedcount over the years, showing a significant increase in research publications after 2000.\n\ngeolocation.csv contains 570 rows and 3 columns, with column names: institution, latitude and longitude. The dataset has no duplicate rows or missing values. The latitude and longitude values span both negative and positive ranges, indicating a global coverage. This suggests that CS rankings vary across different countries worldwide.\n\nturing.csv contains 89 rows and 2 columns, with column names: name and year. The dataset has no duplicate rows or missing values. The award years range from 1966 to 2023. Notably, six researchers received the award in 2002, making it one of the most awarded years."
  },
  {
    "objectID": "media/Stats170A_a1.html#task-2-data-loading-and-transformation",
    "href": "media/Stats170A_a1.html#task-2-data-loading-and-transformation",
    "title": "Data Processing",
    "section": "",
    "text": "We processed all the datasets by reading CSV files, transforming specific columns, and storing them in an SQLite database. One key transformation we performed was splitting names into first_name, middle_name, and last_name for datasets like csrankings.csv and generated-author-info.csv. We achieved this using strsplit(df$name, ‚Äú\\\\s+‚Äù), which separated names based on spaces. The first element was assigned as first_name, while the middle portion was combined into middle_name if present. The last element was usually assigned as last_name, but if the last token was numeric, we used the second-to-last word instead to ensure proper name extraction.\nAdditionally, we enhanced the conference_ranking.csv dataset by categorizing conferences based on their affiliations with ACM, IEEE, or both. Once we completed the transformations, we wrote all datasets into an SQLite database using dbWriteTable(), with queries verifying successful data storage. Lastly for this task, we used dbListTables(con) to confirm that all tables have been created, ensuring a structured and organized database for further analysis.\nconference_ranking.csv (added academic society)\n\ncsrankings.csv (the names were separated into its component first name, middle name and last name values)\n\n\ngenerated-author-info.csv (the names were separated into its component first name, middle name and last name values)\n\nacm-fellows.csv"
  },
  {
    "objectID": "media/Stats170A_a1.html#section",
    "href": "media/Stats170A_a1.html#section",
    "title": "Data Processing",
    "section": "",
    "text": "country-info.csv\n\ndata.csv\n\n\n\n(and 16-26 of 26 columns are not showed for space purpose)\ndblp-aliases.csv\n\nfield_conference.csv\n\ngeolocation.csv\n\nturing.csv"
  },
  {
    "objectID": "media/Stats170A_a1.html#task-3.1",
    "href": "media/Stats170A_a1.html#task-3.1",
    "title": "Data Processing",
    "section": "",
    "text": "The first 10 names from the missing authors list, sorted alphabetically.\n\ngenerated_author_info\n\nacm_fellows\n\nturing\n\n\n\n\nWe combined the three missing authors lists into a single dataframe with distinct names (824 missing names in total). Then we further splitted the name column into first name, middle name, and last name.\n\nFinally, we inserted name, first_name, middle_name, and last_name from missing authors dataframe into csrankings table, other fields values are by default NULL.\n\n\n\n\n\n\n\n\n\nWe use a two-step approach to determine whether two (or more) records from csrankings table represent the same author. First, we define a function that checks if two records have the same last and first name and compatible middle names‚Äîeither an exact match or one name being an initial matching the first letter of the other. For instance, ‚ÄúJohn P. Smith‚Äù and ‚ÄúJohn Paul Smith‚Äù match if they share the same last name, have the same first name, and either matching or initial-matching middle names; we also consider two records duplicates if one or both are missing a middle name but the first and last names match (e.g., ‚ÄúJohn Smith‚Äù and ‚ÄúJohn Paul Smith‚Äù). Second, once we identify groups of authors who are duplicates, we pick one ‚Äúmost complete‚Äù name‚Äîtypically the one with the longest middle name‚Äîto serve as the canonical reference (the ‚Äúsynonym‚Äù) for everyone in that group. This ensures, for example, that ‚ÄúJohn Smith,‚Äù ‚ÄúJohn P Smith,‚Äù and ‚ÄúJohn Paul Smith‚Äù all unify under ‚ÄúJohn Paul Smith.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe following is the representative subsets of the duplicate_author table.\n\n\nThere are a total of 5705 rows of duplicate names being found based on our implementation.\n\n\n\n\nWe retrieved the csrankings and duplicate_author tables from the database, then joined them to replace any duplicate names with their canonical synonyms, standardizing the ‚Äúname‚Äù column. It groups the updated records by this canonical name and merges duplicate rows by selecting, for each attribute, the longest (most complete) value, then writes the resulting clean table back to the database. The cleaned cs_rankings have 28115 rows, compared with cs_rankings 31224 rows, we found and merged 3,109 duplicate rows.\n\n\n\n\nWe update the generated_author_info table by replacing duplicate author names with their canonical synonyms as identified in the duplicate_author table.\nWe deliberately avoid merging rows because we want to preserve the original structure where each row represents a single count entry (e.g., one paper). Merging records would aggregate counts and potentially lose the granularity of the data. By simply replacing the names, we standardize author identifiers across tables while retaining the original row-level detail for further analysis. Therefore, the generated_clean retains 239169 rows."
  },
  {
    "objectID": "media/Stats170A_a1.html#task-4-analysis",
    "href": "media/Stats170A_a1.html#task-4-analysis",
    "title": "Data Processing",
    "section": "",
    "text": "Task 4.1\nSQL Query:\nSELECT\ng.name AS author,\ng.dept AS institution,\nSUM(g.count) AS a_star_publication\nFROM generated_author_info_clean AS g\nJOIN conference_ranking AS c\nON LOWER(g.area) = LOWER(c.Antonym)\nWHERE c.Rank = ‚ÄòA*‚Äô\nAND g.year BETWEEN 2010 AND 2024\nGROUP BY g.name, g.dept\nORDER BY a_star_publication DESC, author ASC, institution ASC\nLIMIT 100;\nOutput:\nFrom the above SQL statement, we have generated top 100 authors. The following screenshot displays the top 10 of them.\n\n\n\n\n\n\n\n\n\nSQL Query:\nWITH RankedInstitutions AS (\nSELECT\nCOALESCE(fc.field, ‚ÄòUnknown Field‚Äô) AS cs_field,\ng.dept AS institution_name,\nCOUNT(*) AS a_star_publications,\nRANK() OVER (\nPARTITION BY COALESCE(fc.field, ‚ÄòUnknown Field‚Äô)\nORDER BY COUNT(*) DESC, g.dept ASC\n) AS rank_position\nFROM generated_author_info_clean g\nJOIN conference_ranking c\nON LOWER(g.area) = LOWER(c.Antonym)\nLEFT JOIN field_conference fc\nON LOWER(g.area) LIKE ‚Äò%‚Äô || LOWER(fc.conference) || ‚Äò%‚Äô ‚Äì Substring matching\nWHERE c.Rank = ‚ÄòA*‚Äô\nGROUP BY COALESCE(fc.field, ‚ÄòUnknown Field‚Äô), g.dept\n)\nSELECT\ncs_field,\ninstitution_name,\na_star_publications\nFROM RankedInstitutions\nWHERE rank_position &lt;= 10\nORDER BY cs_field ASC, a_star_publications DESC, institution_name ASC;\nOutput: From the above SQL statement, we have generated top 10 institutions in each field. The total number of fields is 17."
  }
]